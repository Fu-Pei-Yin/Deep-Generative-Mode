{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOLfK98Cx+J7zHERKKN60T2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fu-Pei-Yin/Deep-Generative-Mode/blob/week2/cGAN%E5%9C%A8%E6%89%8B%E5%AF%AB%E6%95%B8%E5%AD%97%E4%B8%8A%E7%9A%84%E6%87%89%E7%94%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PT4jFeQhxj92",
        "outputId": "f6d30fd0-9756-4cc7-854d-03ec997d344d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Generator parameters: 1,623,312\n",
            "Discriminator parameters: 3,438,593\n",
            "Training start!\n",
            "Epoch [1/50] D_loss: 0.5143 G_loss: 1.7872 D_real_acc: 0.7240 D_fake_acc: 0.8593 Time: 104.73s\n",
            "Epoch [2/50] D_loss: 0.4621 G_loss: 1.8789 D_real_acc: 0.7575 D_fake_acc: 0.9167 Time: 103.88s\n",
            "Epoch [3/50] D_loss: 0.4217 G_loss: 2.0729 D_real_acc: 0.7925 D_fake_acc: 0.9403 Time: 103.19s\n",
            "Epoch [4/50] D_loss: 0.3869 G_loss: 2.4186 D_real_acc: 0.8229 D_fake_acc: 0.9623 Time: 103.85s\n",
            "Epoch [5/50] D_loss: 0.3937 G_loss: 2.4731 D_real_acc: 0.8143 D_fake_acc: 0.9495 Time: 103.23s\n",
            "Epoch [6/50] D_loss: 0.5028 G_loss: 1.7030 D_real_acc: 0.7027 D_fake_acc: 0.8840 Time: 103.39s\n",
            "Epoch [7/50] D_loss: 0.5463 G_loss: 1.4454 D_real_acc: 0.6517 D_fake_acc: 0.8486 Time: 103.08s\n",
            "Epoch [8/50] D_loss: 0.5737 G_loss: 1.2986 D_real_acc: 0.6086 D_fake_acc: 0.8305 Time: 103.93s\n",
            "Epoch [9/50] D_loss: 0.5844 G_loss: 1.2494 D_real_acc: 0.5884 D_fake_acc: 0.8210 Time: 104.49s\n",
            "Epoch [10/50] D_loss: 0.5972 G_loss: 1.2016 D_real_acc: 0.5639 D_fake_acc: 0.8152 Time: 103.26s\n",
            "Epoch [11/50] D_loss: 0.6068 G_loss: 1.1671 D_real_acc: 0.5470 D_fake_acc: 0.8121 Time: 103.52s\n",
            "Epoch [12/50] D_loss: 0.6160 G_loss: 1.1189 D_real_acc: 0.5269 D_fake_acc: 0.8088 Time: 103.36s\n",
            "Epoch [13/50] D_loss: 0.6183 G_loss: 1.1099 D_real_acc: 0.5147 D_fake_acc: 0.8099 Time: 104.64s\n",
            "Epoch [14/50] D_loss: 0.6229 G_loss: 1.0936 D_real_acc: 0.5008 D_fake_acc: 0.8091 Time: 103.50s\n",
            "Epoch [15/50] D_loss: 0.6250 G_loss: 1.0800 D_real_acc: 0.4870 D_fake_acc: 0.8108 Time: 103.19s\n",
            "Epoch [16/50] D_loss: 0.6300 G_loss: 1.0620 D_real_acc: 0.4750 D_fake_acc: 0.8100 Time: 102.56s\n",
            "Epoch [17/50] D_loss: 0.6310 G_loss: 1.0556 D_real_acc: 0.4695 D_fake_acc: 0.8138 Time: 102.69s\n",
            "Epoch [18/50] D_loss: 0.6342 G_loss: 1.0434 D_real_acc: 0.4596 D_fake_acc: 0.8127 Time: 102.42s\n",
            "Epoch [19/50] D_loss: 0.6370 G_loss: 1.0287 D_real_acc: 0.4504 D_fake_acc: 0.8105 Time: 103.61s\n",
            "Epoch [20/50] D_loss: 0.6399 G_loss: 1.0181 D_real_acc: 0.4397 D_fake_acc: 0.8134 Time: 104.67s\n",
            "Epoch [21/50] D_loss: 0.6425 G_loss: 1.0088 D_real_acc: 0.4343 D_fake_acc: 0.8137 Time: 104.16s\n",
            "Epoch [22/50] D_loss: 0.6452 G_loss: 0.9975 D_real_acc: 0.4227 D_fake_acc: 0.8123 Time: 104.43s\n",
            "Epoch [23/50] D_loss: 0.6459 G_loss: 0.9927 D_real_acc: 0.4186 D_fake_acc: 0.8112 Time: 103.23s\n",
            "Epoch [24/50] D_loss: 0.6462 G_loss: 0.9932 D_real_acc: 0.4192 D_fake_acc: 0.8119 Time: 102.30s\n",
            "Epoch [25/50] D_loss: 0.6485 G_loss: 0.9836 D_real_acc: 0.4087 D_fake_acc: 0.8153 Time: 102.65s\n",
            "Epoch [26/50] D_loss: 0.6483 G_loss: 0.9828 D_real_acc: 0.4134 D_fake_acc: 0.8129 Time: 103.02s\n",
            "Epoch [27/50] D_loss: 0.6505 G_loss: 0.9772 D_real_acc: 0.4080 D_fake_acc: 0.8107 Time: 102.67s\n",
            "Epoch [28/50] D_loss: 0.6512 G_loss: 0.9722 D_real_acc: 0.4031 D_fake_acc: 0.8134 Time: 103.15s\n",
            "Epoch [29/50] D_loss: 0.6500 G_loss: 0.9774 D_real_acc: 0.4036 D_fake_acc: 0.8137 Time: 104.25s\n",
            "Epoch [30/50] D_loss: 0.6512 G_loss: 0.9747 D_real_acc: 0.4000 D_fake_acc: 0.8106 Time: 102.43s\n",
            "Epoch [31/50] D_loss: 0.6513 G_loss: 0.9740 D_real_acc: 0.4023 D_fake_acc: 0.8117 Time: 103.05s\n",
            "Epoch [32/50] D_loss: 0.6518 G_loss: 0.9700 D_real_acc: 0.3988 D_fake_acc: 0.8132 Time: 103.08s\n",
            "Epoch [33/50] D_loss: 0.6520 G_loss: 0.9727 D_real_acc: 0.3989 D_fake_acc: 0.8081 Time: 103.14s\n",
            "Epoch [34/50] D_loss: 0.6513 G_loss: 0.9723 D_real_acc: 0.3971 D_fake_acc: 0.8123 Time: 102.88s\n",
            "Epoch [35/50] D_loss: 0.6507 G_loss: 0.9743 D_real_acc: 0.4011 D_fake_acc: 0.8103 Time: 101.96s\n",
            "Epoch [36/50] D_loss: 0.6508 G_loss: 0.9756 D_real_acc: 0.3997 D_fake_acc: 0.8114 Time: 102.14s\n",
            "Epoch [37/50] D_loss: 0.6505 G_loss: 0.9753 D_real_acc: 0.4035 D_fake_acc: 0.8095 Time: 103.81s\n",
            "Epoch [38/50] D_loss: 0.6486 G_loss: 0.9825 D_real_acc: 0.4074 D_fake_acc: 0.8129 Time: 101.52s\n",
            "Epoch [39/50] D_loss: 0.6481 G_loss: 0.9819 D_real_acc: 0.4056 D_fake_acc: 0.8121 Time: 101.42s\n",
            "Epoch [40/50] D_loss: 0.6472 G_loss: 0.9927 D_real_acc: 0.4148 D_fake_acc: 0.8099 Time: 102.62s\n",
            "Epoch [41/50] D_loss: 0.6467 G_loss: 0.9879 D_real_acc: 0.4120 D_fake_acc: 0.8126 Time: 102.31s\n",
            "Epoch [42/50] D_loss: 0.6467 G_loss: 0.9935 D_real_acc: 0.4122 D_fake_acc: 0.8121 Time: 102.57s\n",
            "Epoch [43/50] D_loss: 0.6453 G_loss: 0.9939 D_real_acc: 0.4127 D_fake_acc: 0.8157 Time: 102.45s\n",
            "Epoch [44/50] D_loss: 0.6458 G_loss: 0.9963 D_real_acc: 0.4166 D_fake_acc: 0.8097 Time: 101.28s\n",
            "Epoch [45/50] D_loss: 0.6443 G_loss: 1.0005 D_real_acc: 0.4193 D_fake_acc: 0.8133 Time: 102.36s\n",
            "Epoch [46/50] D_loss: 0.6439 G_loss: 1.0005 D_real_acc: 0.4171 D_fake_acc: 0.8137 Time: 103.10s\n",
            "Epoch [47/50] D_loss: 0.6439 G_loss: 0.9997 D_real_acc: 0.4180 D_fake_acc: 0.8142 Time: 102.97s\n",
            "Epoch [48/50] D_loss: 0.6423 G_loss: 1.0056 D_real_acc: 0.4175 D_fake_acc: 0.8161 Time: 101.40s\n",
            "Epoch [49/50] D_loss: 0.6423 G_loss: 1.0073 D_real_acc: 0.4198 D_fake_acc: 0.8150 Time: 102.10s\n",
            "Epoch [50/50] D_loss: 0.6426 G_loss: 1.0027 D_real_acc: 0.4182 D_fake_acc: 0.8171 Time: 101.97s\n",
            "Total training time: 5372.17 seconds\n",
            "Training finished! Saving models and training history...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1062456273.py:304: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  images.append(imageio.imread(img_name))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All results saved successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import pickle\n",
        "import imageio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "\n",
        "# Generator Network\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim=100, output_dim=784, hidden_dim=256, num_classes=10):\n",
        "        super(Generator, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.fc1_noise = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc1_label = nn.Linear(num_classes, hidden_dim)\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim * 2),\n",
        "            nn.BatchNorm1d(hidden_dim * 2),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim * 4),\n",
        "            nn.BatchNorm1d(hidden_dim * 4),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Linear(hidden_dim * 4, output_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, labels):\n",
        "        # Process noise\n",
        "        noise_out = F.relu(self.fc1_noise(noise))\n",
        "        # Process labels\n",
        "        label_out = F.relu(self.fc1_label(labels))\n",
        "        # Concatenate\n",
        "        x = torch.cat([noise_out, label_out], dim=1)\n",
        "        x = self.main(x)\n",
        "        return x.view(-1, 1, 28, 28)\n",
        "\n",
        "# Discriminator Network\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim=784, hidden_dim=256, num_classes=10):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.fc1_image = nn.Linear(input_dim, hidden_dim * 4)\n",
        "        self.fc1_label = nn.Linear(num_classes, hidden_dim * 4)\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * 8, hidden_dim * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Linear(hidden_dim * 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, images, labels):\n",
        "        # Flatten images\n",
        "        x = images.view(-1, self.input_dim)\n",
        "        # Process images\n",
        "        image_out = F.leaky_relu(self.fc1_image(x), 0.2)\n",
        "        # Process labels\n",
        "        label_out = F.leaky_relu(self.fc1_label(labels), 0.2)\n",
        "        # Concatenate\n",
        "        x = torch.cat([image_out, label_out], dim=1)\n",
        "        x = self.main(x)\n",
        "        return x.squeeze()\n",
        "\n",
        "# 設置設備\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# 準備固定輸入用於可視化 (生成數字0-9各10張)\n",
        "def create_fixed_noise_and_labels():\n",
        "    fixed_noise = torch.randn(100, 100).to(device)\n",
        "    fixed_labels = torch.zeros(100, 10).to(device)\n",
        "\n",
        "    # 創建標籤: 數字0-9各10個\n",
        "    for digit in range(10):\n",
        "        for i in range(10):\n",
        "            idx = digit * 10 + i\n",
        "            fixed_labels[idx, digit] = 1.0\n",
        "\n",
        "    return fixed_noise, fixed_labels\n",
        "\n",
        "fixed_noise, fixed_labels = create_fixed_noise_and_labels()\n",
        "\n",
        "def show_result(num_epoch, show=False, save=False, path='result.png'):\n",
        "    G.eval()\n",
        "    with torch.no_grad():\n",
        "        test_images = G(fixed_noise, fixed_labels)\n",
        "    G.train()\n",
        "\n",
        "    fig, axes = plt.subplots(10, 10, figsize=(10, 10))\n",
        "    for i in range(10):\n",
        "        for j in range(10):\n",
        "            idx = i * 10 + j\n",
        "            axes[i, j].imshow(test_images[idx].cpu().squeeze().numpy(), cmap='gray')\n",
        "            axes[i, j].axis('off')\n",
        "            axes[i, j].set_title(f'{i}', fontsize=8)\n",
        "\n",
        "    plt.suptitle(f'cGAN Generated Images - Epoch {num_epoch}', fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(path, dpi=150, bbox_inches='tight')\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "def show_train_hist(hist, show=False, save=False, path='Train_hist.png'):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(hist['D_losses'], label='Discriminator Loss')\n",
        "    plt.plot(hist['G_losses'], label='Generator Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training Losses')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(hist['D_real_acc'], label='Real Accuracy')\n",
        "    plt.plot(hist['D_fake_acc'], label='Fake Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Discriminator Accuracy')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "# 訓練參數\n",
        "batch_size = 128\n",
        "lr = 0.0002  # 2e-4 as required\n",
        "train_epoch = 50\n",
        "label_smooth = 0.1  # Label smoothing factor\n",
        "\n",
        "# 資料載入\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 初始化網絡\n",
        "G = Generator().to(device)\n",
        "D = Discriminator().to(device)\n",
        "\n",
        "print(f\"Generator parameters: {sum(p.numel() for p in G.parameters()):,}\")\n",
        "print(f\"Discriminator parameters: {sum(p.numel() for p in D.parameters()):,}\")\n",
        "\n",
        "# 損失函數和優化器\n",
        "criterion = nn.BCELoss()\n",
        "G_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "D_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "# 建立結果資料夾\n",
        "os.makedirs('MNIST_cGAN_results/Fixed_results', exist_ok=True)\n",
        "\n",
        "# 訓練歷史記錄\n",
        "train_hist = {}\n",
        "train_hist['D_losses'] = []\n",
        "train_hist['G_losses'] = []\n",
        "train_hist['D_real_acc'] = []\n",
        "train_hist['D_fake_acc'] = []\n",
        "train_hist['per_epoch_ptimes'] = []\n",
        "train_hist['total_ptime'] = []\n",
        "\n",
        "print('Training start!')\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(train_epoch):\n",
        "    D_losses = []\n",
        "    G_losses = []\n",
        "    D_real_accuracies = []\n",
        "    D_fake_accuracies = []\n",
        "\n",
        "    epoch_start_time = time.time()\n",
        "\n",
        "    for batch_idx, (real_images, real_labels) in enumerate(train_loader):\n",
        "        batch_size = real_images.size(0)\n",
        "\n",
        "        # 準備真實資料\n",
        "        real_images = real_images.to(device)\n",
        "        real_labels_onehot = F.one_hot(real_labels, num_classes=10).float().to(device)\n",
        "\n",
        "        # 使用標籤平滑\n",
        "        real_targets = torch.full((batch_size,), 1.0 - label_smooth, device=device)\n",
        "        fake_targets = torch.zeros(batch_size, device=device)\n",
        "\n",
        "        # 訓練判別器\n",
        "        D.zero_grad()\n",
        "\n",
        "        # 真實圖片損失\n",
        "        D_real_output = D(real_images, real_labels_onehot)\n",
        "        D_real_loss = criterion(D_real_output, real_targets)\n",
        "        D_real_accuracy = (D_real_output > 0.5).float().mean()\n",
        "\n",
        "        # 生成假圖片\n",
        "        noise = torch.randn(batch_size, 100, device=device)\n",
        "        fake_labels = torch.randint(0, 10, (batch_size,), device=device)\n",
        "        fake_labels_onehot = F.one_hot(fake_labels, num_classes=10).float().to(device)\n",
        "\n",
        "        fake_images = G(noise, fake_labels_onehot)\n",
        "\n",
        "        # 假圖片損失\n",
        "        D_fake_output = D(fake_images.detach(), fake_labels_onehot)\n",
        "        D_fake_loss = criterion(D_fake_output, fake_targets)\n",
        "        D_fake_accuracy = (D_fake_output < 0.5).float().mean()\n",
        "\n",
        "        # 總判別器損失\n",
        "        D_total_loss = (D_real_loss + D_fake_loss) / 2\n",
        "        D_total_loss.backward()\n",
        "        D_optimizer.step()\n",
        "\n",
        "        # 訓練生成器\n",
        "        G.zero_grad()\n",
        "\n",
        "        D_fake_output = D(fake_images, fake_labels_onehot)\n",
        "        G_loss = criterion(D_fake_output, real_targets)  # 騙過判別器\n",
        "\n",
        "        G_loss.backward()\n",
        "        G_optimizer.step()\n",
        "\n",
        "        # 記錄損失和準確率\n",
        "        D_losses.append(D_total_loss.item())\n",
        "        G_losses.append(G_loss.item())\n",
        "        D_real_accuracies.append(D_real_accuracy.item())\n",
        "        D_fake_accuracies.append(D_fake_accuracy.item())\n",
        "\n",
        "    epoch_end_time = time.time()\n",
        "    per_epoch_ptime = epoch_end_time - epoch_start_time\n",
        "\n",
        "    # 計算epoch平均\n",
        "    avg_D_loss = np.mean(D_losses)\n",
        "    avg_G_loss = np.mean(G_losses)\n",
        "    avg_D_real_acc = np.mean(D_real_accuracies)\n",
        "    avg_D_fake_acc = np.mean(D_fake_accuracies)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{train_epoch}] '\n",
        "          f'D_loss: {avg_D_loss:.4f} '\n",
        "          f'G_loss: {avg_G_loss:.4f} '\n",
        "          f'D_real_acc: {avg_D_real_acc:.4f} '\n",
        "          f'D_fake_acc: {avg_D_fake_acc:.4f} '\n",
        "          f'Time: {per_epoch_ptime:.2f}s')\n",
        "\n",
        "    # 保存結果\n",
        "    fixed_p = f'MNIST_cGAN_results/Fixed_results/MNIST_cGAN_{epoch+1}.png'\n",
        "    show_result(epoch+1, save=True, path=fixed_p)\n",
        "\n",
        "    train_hist['D_losses'].append(avg_D_loss)\n",
        "    train_hist['G_losses'].append(avg_G_loss)\n",
        "    train_hist['D_real_acc'].append(avg_D_real_acc)\n",
        "    train_hist['D_fake_acc'].append(avg_D_fake_acc)\n",
        "    train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n",
        "\n",
        "end_time = time.time()\n",
        "total_ptime = end_time - start_time\n",
        "train_hist['total_ptime'].append(total_ptime)\n",
        "\n",
        "print(f\"Total training time: {total_ptime:.2f} seconds\")\n",
        "print(\"Training finished! Saving models and training history...\")\n",
        "\n",
        "# 保存模型和訓練歷史\n",
        "torch.save(G.state_dict(), \"MNIST_cGAN_results/generator_param.pkl\")\n",
        "torch.save(D.state_dict(), \"MNIST_cGAN_results/discriminator_param.pkl\")\n",
        "\n",
        "with open('MNIST_cGAN_results/train_hist.pkl', 'wb') as f:\n",
        "    pickle.dump(train_hist, f)\n",
        "\n",
        "# 生成訓練過程圖表和動畫\n",
        "show_train_hist(train_hist, save=True, path='MNIST_cGAN_results/MNIST_cGAN_train_hist.png')\n",
        "\n",
        "images = []\n",
        "for e in range(train_epoch):\n",
        "    img_name = f'MNIST_cGAN_results/Fixed_results/MNIST_cGAN_{e+1}.png'\n",
        "    if os.path.exists(img_name):\n",
        "        images.append(imageio.imread(img_name))\n",
        "imageio.mimsave('MNIST_cGAN_results/generation_animation.gif', images, fps=5)\n",
        "\n",
        "print(\"All results saved successfully!\")"
      ]
    }
  ]
}